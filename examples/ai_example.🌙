﷽:
    from ai import load_model, generate_text, create_chat_completion

    # Load the GGUF model with specific parameters
    model_params = {
        "n_ctx": 4096,          # Larger context window
        "n_batch": 512,         # Batch size for prompt processing
        "n_threads": 8,         # Use more CPU threads
        "n_gpu_layers": 35,     # Number of layers to offload to GPU
        "f16_kv": True,         # Use FP16 for key/value cache
        "use_mmap": True,       # Use memory mapping
        "use_mlock": True,      # Keep model in RAM
        "embedding": False      # Don't extract embeddings
    } ۝
    
    model = load_model("models/islamic_model.gguf", model_params) ۝

    # Configure generation parameters
    gen_params = {
        "temperature": 0.7,     # Lower temperature for more focused output
        "top_p": 0.9,          # Nucleus sampling
        "top_k": 40,           # Top-k sampling
        "max_tokens": 2048,    # Maximum response length
        "repeat_penalty": 1.1,  # Penalize repetition
        "presence_penalty": 0.0,
        "frequency_penalty": 0.0,
        "stop": ["User:", "Assistant:", "\n\n"]
    } ۝

    # Generate text using the model
    prompt = """
    Explain the concept of Tawheed (Islamic monotheism) in simple terms,
    focusing on its importance in Islamic theology.
    """ ۝
    
    response = generate_text(model, prompt, gen_params) ۝
    print("Generated Response:", response) ۝

    # Use chat completion for interactive dialogue
    messages = [
        {
            "role": "system",
            "content": "You are a knowledgeable Islamic scholar who provides accurate and respectful information about Islam."
        },
        {
            "role": "user",
            "content": "What are the five pillars of Islam and why are they important?"
        }
    ] ۝
    
    chat_response = create_chat_completion(model, messages, gen_params) ۝
    print("\nChat Response:", chat_response) ۝

    # Error handling example
    try:
        invalid_model = load_model("nonexistent.gguf") ۝
    except ValueError as e:
        print("Error:", str(e)) ۝
